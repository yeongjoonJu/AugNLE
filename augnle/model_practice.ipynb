{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pr03/anaconda3/envs/t5/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/pr03/anaconda3/envs/t5/lib/python3.7/site-packages/transformers/models/t5/tokenization_t5.py:169: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import T5Tokenizer\n",
    "import json\n",
    "import torch\n",
    "import torch\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "img_size = 224\n",
    "vqax_data_dir = \"/media/storage/coco/VQA-X/annotated/vqaX_train.json\"\n",
    "coco_data_dir = \"/media/storage/coco/\"\n",
    "input_max_seq_length = 500\n",
    "output_max_seq_length = 30\n",
    "train_batch_size = 30\n",
    "eval_batch_size =  30\n",
    "img_transform = transforms.Compose([transforms.Resize((img_size,img_size)), transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "task_A = True\n",
    "is_train = True\n",
    "from PIL import Image\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
    "num_new_tokens = tokenizer.add_special_tokens({'pad_token': '<pad>','additional_special_tokens': ['<question>', '<situation>', '<answer>']})\n",
    "\n",
    "data = json.load(open(vqax_data_dir, 'r'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VQA-X data preprocessing...: 100%|██████████| 30/30 [00:00<00:00, 57.92it/s]\n"
     ]
    }
   ],
   "source": [
    "ids_list = list(data.keys())\n",
    "index_tracker = {k: len(v['explanation']) - 1 for k,v in data.items()}\n",
    "for k,v in data.items():\n",
    "    if len(v['explanation']) > 1:   # some questions have more than one explanation\n",
    "# duplicate them for loading. -1 because one explanation is already in ids_list\n",
    "        ids_list += [str(k)] * (len(v['explanation']) - 1)\n",
    "        \n",
    "datasets = []\n",
    "for i in tqdm(range(30), desc= \"VQA-X data preprocessing...\"):\n",
    "    quention_id = ids_list[i]\n",
    "    sample = data[quention_id]\n",
    "    img_name = sample['image_name']\n",
    "\n",
    "    text_q = utils.proc_ques(sample['question'])    # question\n",
    "    text_a = utils.proc_ans(sample['answers'])\n",
    "    exp_idx = index_tracker[quention_id]\n",
    "    text_e = sample['explanation'][exp_idx]\n",
    "\n",
    "    # 2개의 explanation 이라면\n",
    "    if exp_idx > 0:\n",
    "        index_tracker[quention_id] -= 1    # decrease usage\n",
    "        \n",
    "    q_segment_id, s_segment_id, a_segment_id, e_segment_id = tokenizer.convert_tokens_to_ids(['<question>', '<situation>', '<answer>', '<explanation>'])\n",
    "\n",
    "    question_tokens = tokenizer.tokenize(text_q)\n",
    "    segment_ids = [q_segment_id] * len(question_tokens)\n",
    "\n",
    "    # situation\n",
    "    situation_tag = tokenizer.tokenize(\"situation:\")\n",
    "\n",
    "    # answer\n",
    "    answer_tokens =  tokenizer.tokenize(text_a)\n",
    "    answer_tag = tokenizer.tokenize(\"answer:\")\n",
    "    answer_len = len(answer_tokens)\n",
    "    answer_tokens\n",
    "\n",
    "    # explanation\n",
    "    explanation_tokens = tokenizer.tokenize(text_e)\n",
    "    explanation_tag = tokenizer.tokenize(\"explanation:\")\n",
    "    exp_len = len(explanation_tokens)\n",
    "\n",
    "    # Task A -> Q, I, A -> E\n",
    "    if task_A:\n",
    "        tokens = question_tokens + situation_tag + [\"<situation>\"]*196 + answer_tag + answer_tokens + [tokenizer.eos_token]\n",
    "        segment_ids = segment_ids + [s_segment_id]*196 + [e_segment_id] * (len(answer_tokens) +2)\n",
    "        labels = explanation_tokens + [tokenizer.eos_token]\n",
    "\n",
    "    # Task B -> Q, E -> A\n",
    "    else:\n",
    "        tokens = question_tokens + explanation_tag + explanation_tokens + [tokenizer.eos_token]\n",
    "        segment_ids = segment_ids + [e_segment_id] * (len(tokens) - len(segment_ids))\n",
    "        labels = answer_tokens + [tokenizer.eos_token]\n",
    "        \n",
    "    # Split over sequence length\n",
    "    if len(tokens) > input_max_seq_length :\n",
    "        tokens = tokens[:input_max_seq_length]\n",
    "        segment_ids = segment_ids[:input_max_seq_length]\n",
    "        \n",
    "    # Padding\n",
    "    seq_len = len(tokens)\n",
    "    input_padding_len = input_max_seq_length - len(tokens)\n",
    "    output_padding_len = output_max_seq_length - len(labels)\n",
    "    tokens = tokens + ([tokenizer.pad_token] * input_padding_len)\n",
    "    labels = labels + ([tokenizer.pad_token] * output_padding_len)\n",
    "    segment_ids += ([e_segment_id] * input_padding_len)\n",
    "    # token to ids\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "\n",
    "    labels = [tokenizer.convert_tokens_to_ids(t) for t in labels]\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    segment_ids = torch.tensor(segment_ids, dtype=torch.long)\n",
    "\n",
    "\n",
    "    ## Image\n",
    "\n",
    "    folder = coco_data_dir + '/train2014/' if 'train' in img_name else coco_data_dir + 'val2014/'\n",
    "    img_path = folder + img_name\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img = img_transform(img)\n",
    "    qid =quention_id\n",
    "    qid = torch.LongTensor([int(quention_id)])\n",
    "\n",
    "    datasets.append({\"img\": img, \"qid\" : qid, \"input_ids\": input_ids, \"labels\": labels, \"segment_ids\" : segment_ids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5185, -1.5357, -1.7069,  ...,  0.5707,  0.7933,  0.8276],\n",
       "         [-1.7412, -1.6898, -1.6555,  ...,  0.4508,  0.7248,  0.8447],\n",
       "         [-1.8097, -1.7412, -1.6384,  ..., -0.2342,  0.0741,  0.3481],\n",
       "         ...,\n",
       "         [-0.0629,  0.0569,  0.0912,  ...,  0.2796,  0.2796,  0.2967],\n",
       "         [ 0.1768,  0.3823,  0.3138,  ...,  0.3138,  0.3309,  0.3309],\n",
       "         [ 0.5193,  0.4679,  0.1426,  ...,  0.2796,  0.2967,  0.2796]],\n",
       "\n",
       "        [[-1.3880, -1.4230, -1.5980,  ...,  0.7129,  0.8880,  0.9230],\n",
       "         [-1.6155, -1.5805, -1.5280,  ...,  0.6078,  0.8704,  0.9755],\n",
       "         [-1.7031, -1.6331, -1.5105,  ..., -0.0749,  0.2052,  0.4678],\n",
       "         ...,\n",
       "         [ 0.0826,  0.2052,  0.2402,  ...,  0.4328,  0.4328,  0.4503],\n",
       "         [ 0.3102,  0.5203,  0.4503,  ...,  0.4678,  0.4678,  0.4853],\n",
       "         [ 0.6429,  0.5903,  0.2752,  ...,  0.4328,  0.4328,  0.4503]],\n",
       "\n",
       "        [[-1.3339, -1.3861, -1.4733,  ...,  0.5834,  0.7751,  0.8099],\n",
       "         [-1.4907, -1.4733, -1.4384,  ...,  0.5659,  0.7751,  0.8622],\n",
       "         [-1.5779, -1.5430, -1.4907,  ..., -0.0441,  0.1825,  0.4265],\n",
       "         ...,\n",
       "         [ 0.2173,  0.3045,  0.3393,  ...,  0.5485,  0.5485,  0.5485],\n",
       "         [ 0.4439,  0.5659,  0.4962,  ...,  0.5834,  0.5834,  0.5659],\n",
       "         [ 0.7751,  0.6879,  0.3742,  ...,  0.5485,  0.5659,  0.5485]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0][\"img\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, device):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.encoder, _ = clip.load(\"ViT-B/16\", device= device)   # loads already in eval mode\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Expects a tensor of size (batch_size, 3, 224, 224)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            x = x.type(self.encoder.visual.conv1.weight.dtype)\n",
    "            # Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
    "            x = self.encoder.visual.conv1(x)  # shape = [*, width, grid, grid]\n",
    "            print(\"1\",x.shape)\n",
    "            x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n",
    "            print(\"2\",x.shape)\n",
    "            x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n",
    "            print(\"3\",x.shape)\n",
    "            x = torch.cat([self.encoder.visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n",
    "            print(\"4\",x.shape)\n",
    "            x = x + self.encoder.visual.positional_embedding.to(x.dtype)\n",
    "            print(\"5\",x.shape)\n",
    "            x = self.encoder.visual.ln_pre(x)\n",
    "            print(\"6\",x.shape)\n",
    "            x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "            print(\"7\",x.shape)\n",
    "            x = self.encoder.visual.transformer(x)\n",
    "            print(\"8\",x.shape)\n",
    "            grid_feats = x.permute(1, 0, 2)  # LND -> NLD    (N, 197, 768)\n",
    "            print(\"9\",x.shape)\n",
    "            grid_feats = self.encoder.visual.ln_post(grid_feats[:,1:])  \n",
    "            print(grid_feats.shape)\n",
    "                \n",
    "        return grid_feats.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = ImageEncoder(torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 torch.Size([1, 768, 14, 14])\n",
      "2 torch.Size([1, 768, 196])\n",
      "3 torch.Size([1, 196, 768])\n",
      "4 torch.Size([1, 197, 768])\n",
      "5 torch.Size([1, 197, 768])\n",
      "6 torch.Size([1, 197, 768])\n",
      "7 torch.Size([197, 1, 768])\n",
      "8 torch.Size([197, 1, 768])\n",
      "9 torch.Size([197, 1, 768])\n",
      "torch.Size([1, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "samp = datasets[0][\"img\"].unsqueeze(0)\n",
    "output = enc(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, _ = clip.load(\"ViT-B/16\", device= torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(self.encoder.visual.conv1.weight.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.visual.conv1.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.4430e-04, -4.2040e-02,  6.1855e-02,  ...,  7.9190e-02,\n",
       "          7.7290e-02,  3.2362e-06],\n",
       "        [-8.7270e-03, -1.6116e-02,  3.5423e-02,  ..., -4.7097e-02,\n",
       "         -4.1042e-02,  7.3858e-02],\n",
       "        [-9.3716e-03, -1.6571e-02,  9.9012e-03,  ..., -3.6754e-02,\n",
       "         -2.3377e-02,  2.3383e-02],\n",
       "        ...,\n",
       "        [ 2.9686e-03, -1.2590e-02, -4.1563e-02,  ..., -4.5710e-02,\n",
       "         -4.2346e-02,  7.2440e-03],\n",
       "        [ 4.9054e-03, -1.8691e-02,  2.8315e-03,  ..., -4.2236e-02,\n",
       "         -3.7692e-02, -1.6221e-03],\n",
       "        [ 6.3653e-03, -2.3058e-02, -5.4040e-02,  ...,  3.0824e-02,\n",
       "         -4.3921e-02,  5.2905e-02]], requires_grad=True)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.visual.positional_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 768])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0]['img'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import T5ForConditionalGeneration, T5Config\n",
    "\n",
    "from models.prefix_encoder import PrefixEncoder\n",
    "\n",
    "class T5PrefixForConditionalGeneration(T5ForConditionalGeneration):\n",
    "    def __init__(self, config: T5Config):\n",
    "        super().__init__(config)\n",
    "        self.prefix_seq_len = config.prefix_seq_len\n",
    "        self.n_layers = config.num_hidden_layers\n",
    "        self.n_heads = config.num_attention_heads\n",
    "        self.n_embeds = config.hidden_size // config.num_attention_heads\n",
    "        print(self.n_embeds)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.prefix_encoder = PrefixEncoder(config)\n",
    "        self.prefix_tokens = torch.arange(self.prefix_seq_len).long()\n",
    "\n",
    "    def get_prompt(self, batch_size):\n",
    "        prefix_tokens = self.prefix_tokens.unsqueeze(0).expand(batch_size, -1).to(self.device)\n",
    "        past_key_values = self.prefix_encoder(prefix_tokens)\n",
    "        bsz, seq_len, _ = past_key_values.shape\n",
    "        past_key_values = past_key_values.view(bsz, seq_len, self.n_layers*2, self.n_heads, self.n_embeds)\n",
    "        past_key_values = self.dropout(past_key_values)\n",
    "        past_key_values = past_key_values.permute([2,0,3,1,4]).split(2)\n",
    "\n",
    "        return past_key_values\n",
    "    \n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                decoder_attention_mask=None,\n",
    "                decoder_input_ids=None,\n",
    "                encoder_outputs=None,\n",
    "                past_key_values=None,\n",
    "                inputs_embeds=None,\n",
    "                decoder_inputs_embeds=None,\n",
    "                labels=None,\n",
    "                use_cache=None,\n",
    "                output_attentions=None,\n",
    "                output_hidden_states=None,\n",
    "                return_dict=None,\n",
    "                encoder_only=None,):\n",
    "        \n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.return_dict\n",
    "        \n",
    "        batch_size = input_ids.shape[0]\n",
    "        #enc_past_key_values = self.get_prompt(batch_size=batch_size)\n",
    "        #prefix_attention_mask = torch.ones(batch_size, self.prefix_seq_len).to(self.device)\n",
    "        #attention_mask = torch.cat((prefix_attention_mask, attention_mask), dim=1)\n",
    "        #print(input_ids)\n",
    "        #print(inputs_embeds)\n",
    "        input_ids_emb = self.shared(input_ids)\n",
    "        print(input_ids_emb)\n",
    "        print(input_ids_emb.shape)\n",
    "        return input_ids_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "conf = T5Config.from_pretrained(\"t5-large\")\n",
    "conf.prefix_seq_len = 30\n",
    "conf.hidden_dropout_prob = 0.1\n",
    "conf.prefix_projection = True\n",
    "conf.pre_seq_len = 30\n",
    "conf.prefix_hidden_size = 30\n",
    "model = T5PrefixForConditionalGeneration(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 500])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_sample = datasets[0][\"input_ids\"].unsqueeze(0)\n",
    "ids_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.7534, -0.6862, -0.3778,  ..., -1.2773,  0.8081, -0.6258],\n",
      "         [-1.1070, -0.3546, -0.0654,  ..., -2.2251,  1.3561,  1.2644],\n",
      "         [ 0.6979, -1.5705,  1.1215,  ...,  0.3965, -0.6999,  1.4335],\n",
      "         ...,\n",
      "         [ 0.5292, -0.7427, -1.8560,  ...,  0.0856, -0.9413, -1.1828],\n",
      "         [ 0.5292, -0.7427, -1.8560,  ...,  0.0856, -0.9413, -1.1828],\n",
      "         [ 0.5292, -0.7427, -1.8560,  ...,  0.0856, -0.9413, -1.1828]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([1, 500, 1024])\n"
     ]
    }
   ],
   "source": [
    "test1 = model(input_ids= ids_sample, inputs_embeds = output )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "situation_token = tokenizer.encode(\"<situation>\")[0]\n",
    "situation_idx = []\n",
    "for i in ids_sample:\n",
    "    for num,j in enumerate(i):\n",
    "        if j == torch.Tensor([situation_token]):\n",
    "            situation_idx.append(num)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4426,  0.9423, -2.4927,  ...,  0.6239,  0.5189,  1.2511],\n",
       "        [-0.8097, -0.4102,  0.3188,  ...,  2.0389,  0.5652, -0.0307],\n",
       "        [ 1.2144,  0.5852,  0.0984,  ..., -0.0879, -0.0627, -2.0041],\n",
       "        ...,\n",
       "        [-0.3181,  0.0744,  0.1203,  ...,  0.5819, -0.5419,  0.2743],\n",
       "        [-0.3181,  0.0744,  0.1203,  ...,  0.5819, -0.5419,  0.2743],\n",
       "        [-0.3181,  0.0744,  0.1203,  ...,  0.5819, -0.5419,  0.2743]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp1 = nn.Linear(768,1024)\n",
    "mlp2 = nn.Linear(1024,1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 196, 1024])\n"
     ]
    }
   ],
   "source": [
    "output2 = mlp1(output)\n",
    "print(output2.shape)\n",
    "output2 = mlp2(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,location in enumerate(situation_idx):\n",
    "    test1[idx][location:location+196,:] = output2[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3828,  0.3313, -0.7732,  ..., -0.2175, -0.1339,  0.8477],\n",
       "         [ 0.3436, -0.1190,  0.3153,  ..., -0.6236, -1.0854, -1.2386],\n",
       "         [-0.2829,  0.5944,  0.4410,  ..., -0.5823, -0.2191, -1.0603],\n",
       "         ...,\n",
       "         [ 0.3141, -0.1474,  0.0801,  ...,  0.2868, -1.0450, -0.7127],\n",
       "         [ 0.3141, -0.1474,  0.0801,  ...,  0.2868, -1.0450, -0.7127],\n",
       "         [ 0.3141, -0.1474,  0.0801,  ...,  0.2868, -1.0450, -0.7127]]],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "':'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('<situation>')[0]\n",
    "tokenizer.decode(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4390, -0.9218,  1.2406,  ..., -0.8661, -0.8282, -0.2915],\n",
      "         [ 1.0726, -0.9998, -0.7318,  ..., -2.1169,  0.0718, -0.4649],\n",
      "         [-0.3517, -1.5154, -0.9226,  ..., -0.4945, -1.1152, -1.1642],\n",
      "         ...,\n",
      "         [ 0.9891, -0.0211,  1.2571,  ..., -0.0514, -0.2245, -1.2818],\n",
      "         [ 0.9891, -0.0211,  1.2571,  ..., -0.0514, -0.2245, -1.2818],\n",
      "         [ 0.9891, -0.0211,  1.2571,  ..., -0.0514, -0.2245, -1.2818]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([1, 500, 1024])\n"
     ]
    }
   ],
   "source": [
    "test1 = model(input_ids= ids_sample, inputs_embeds = output )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['img', 'qid', 'input_ids', 'labels', 'segment_ids'])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18483,     1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 0, 1, 2, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "batch_size = 4\n",
    "n_views = 2\n",
    "labels = torch.cat([torch.arange(batch_size) for i in range(n_views)], dim=0)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.eye(labels.shape[0], dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False, False, False, False, False],\n",
       "        [False,  True, False, False, False, False, False, False],\n",
       "        [False, False,  True, False, False, False, False, False],\n",
       "        [False, False, False,  True, False, False, False, False],\n",
       "        [False, False, False, False,  True, False, False, False],\n",
       "        [False, False, False, False, False,  True, False, False],\n",
       "        [False, False, False, False, False, False,  True, False],\n",
       "        [False, False, False, False, False, False, False,  True]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = labels[~mask].view(labels.shape[0], -1)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "920bff935c1399c8ca1f22b0319a3cb298b983f35fbad2f58be88cbfac3be712"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('t5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
