{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import GPT2Tokenizer, AutoConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import json\n",
    "from self_training.cococaption.pycocotools.coco import COCO\n",
    "from self_training.cococaption.pycocoevalcap.eval import COCOEvalCap\n",
    "from PIL import Image\n",
    "from accelerate import Accelerator\n",
    "from self_training import data_utils\n",
    "from self_training.models.gpt import GPT2LMHeadModel\n",
    "from self_training.eval_utils import top_filtering\n",
    "import self_training.models.clip_x.clip as clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Path Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_save_path = '../self_training/cococaption/results/' \n",
    "annFileExp = '../self_training/cococaption/annotations/vqaX_test_annot_exp.json'\n",
    "annFileFull = '../self_training/cococaption/annotations/vqaX_test_annot_full.json'\n",
    "nle_data_test_path = '../self_training/nle_data/VQA-X/vqaX_test.json'\n",
    "nle_data_val_path = '../self_training/nle_data/VQA-X/vqaX_val.json'\n",
    "nle_data_train_path = '../self_training/nle_data/VQA-X/vqaX_train.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(ckpt_path, epoch):\n",
    "    \n",
    "    model_name = 'nle_model_{}'.format(str(epoch))\n",
    "    tokenizer_name = 'nle_gpt2_tokenizer_0'\n",
    "    filename = 'ckpt_stats_' + str(epoch) + '.tar'\n",
    "    \n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(ckpt_path + tokenizer_name)        # load tokenizer\n",
    "    model = GPT2LMHeadModel.from_pretrained(ckpt_path + model_name).to(device)   # load model with config\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "def change_requires_grad(model, req_grad):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = req_grad\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "clip._MODELS = {\n",
    "    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n",
    "    \"ViT-B/16\": \"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\",\n",
    "    \"ViT-L/14\": \"https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt\"\n",
    "}\n",
    "\n",
    "clip_model, preprocess = clip.load(\"ViT-B/16\", device=device, jit=False)\n",
    "print(\"Image resolution:\", clip_model.visual.input_resolution)\n",
    "image_encoder = clip_model\n",
    "# change_requires_grad(image_encoder, False)\n",
    "img_transform = preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elements(question_id):\n",
    "    sample = data[question_id]\n",
    "    img_name = sample['image_name']\n",
    "    text_a = data_utils.proc_ques(sample['question'])    # question\n",
    "\n",
    "    # tokenization process\n",
    "    q_segment_id, a_segment_id, e_segment_id = tokenizer.convert_tokens_to_ids(['<question>', '<answer>', '<explanation>'])\n",
    "    tokens = tokenizer.tokenize(text_a)\n",
    "    segment_ids = [q_segment_id] * len(tokens)\n",
    "\n",
    "    answer = [tokenizer.bos_token] + tokenizer.tokenize(\" the answer is\")\n",
    "    answer_len = len(answer)\n",
    "    tokens += answer \n",
    "\n",
    "    segment_ids += [a_segment_id] * answer_len\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "    segment_ids = torch.tensor(segment_ids, dtype=torch.long)\n",
    "\n",
    "    folder = '../self_training/images/train2014/' if 'train' in img_name else '../self_training/images/val2014/'   # test and val are both in val2014\n",
    "    img_path = folder + img_name\n",
    "    img = Image.open(img_path)\n",
    "    #.convert('RGB')\n",
    "    img = img_transform(img)\n",
    "    qid = torch.LongTensor([int(question_id)])\n",
    "\n",
    "    return (img, qid, input_ids, segment_ids, img_path)\n",
    "\n",
    "# data list\n",
    "data = json.load(open(nle_data_test_path, \"r\"))\n",
    "ids_list = list(data.keys())\n",
    "len(ids_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Interability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2, math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from self_training.models.gpt import NLX_GPT\n",
    "\n",
    "\n",
    "load_from_epoch = 11\n",
    "ckpt_path = '../self_training/ckpts/VQAX_p/'\n",
    "\n",
    "tokenizer, model = load_checkpoint(ckpt_path, load_from_epoch)\n",
    "\n",
    "SPECIAL_TOKENS = ['<|endoftext|>', '<pad>', '<question>', '<answer>', '<explanation>']\n",
    "special_tokens_ids = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n",
    "because_token_id = tokenizer.convert_tokens_to_ids('Ä because')\n",
    "\n",
    "\n",
    "nlx_gpt = NLX_GPT(visual_backbone=image_encoder, lm_backbone=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_relevance(image_relevance, image, orig_image):\n",
    "    # create heatmap from mask on image\n",
    "    def show_cam_on_image(img, mask):\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "        heatmap = np.float32(heatmap) / 255\n",
    "        cam = heatmap + np.float32(img)\n",
    "        cam = cam / np.max(cam)\n",
    "        return cam\n",
    "\n",
    "    # plt.axis('off')\n",
    "    # f, axarr = plt.subplots(1,2)\n",
    "    # axarr[0].imshow(orig_image)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    axs[0].imshow(orig_image);\n",
    "    axs[0].axis('off');\n",
    "    \n",
    "    feat_hw = int(math.sqrt(image_relevance.shape[-1]))\n",
    "    image_relevance = image_relevance.reshape(1, 1, feat_hw, feat_hw)\n",
    "    image_relevance = torch.nn.functional.interpolate(image_relevance, size=224, mode='bilinear')\n",
    "    image_relevance = image_relevance.reshape(224, 224).cuda().data.cpu().numpy()\n",
    "    image_relevance = (image_relevance - image_relevance.min()) / (image_relevance.max() - image_relevance.min())\n",
    "    image = image[0].permute(1, 2, 0).data.cpu().numpy()\n",
    "    image = (image - image.min()) / (image.max() - image.min())\n",
    "    vis = show_cam_on_image(image, image_relevance)\n",
    "    vis = np.uint8(255 * vis)\n",
    "    vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
    "    # axar[1].imshow(vis)\n",
    "    axs[1].imshow(vis);\n",
    "    axs[1].axis('off');\n",
    "    # plt.imshow(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import top_k_top_p_filtering\n",
    "\n",
    "img_size = 224\n",
    "max_seq_len = 40\n",
    "do_sample = False\n",
    "top_k =  0\n",
    "top_p =  0.9\n",
    "temperature = 1\n",
    "\n",
    "start_layer = 11\n",
    "\n",
    "q_id = ids_list[0]\n",
    "batch = get_elements(q_id)\n",
    "img_path = batch[-1]\n",
    "batch = tuple(input_tensor.unsqueeze(0).to(device) for input_tensor in batch[:-1])\n",
    "img, img_id, input_ids, segment_ids = batch\n",
    "batch_size = img.shape[0]\n",
    "\n",
    "image_attn_blocks = list(dict(nlx_gpt.visual_encoder.visual.transformer.resblocks.named_children()).values())\n",
    "num_tokens = image_attn_blocks[0].attn_probs.shape[-1]\n",
    "\n",
    "img_relevance_maps = []\n",
    "current_output = []\n",
    "current_logits = []\n",
    "always_exp = False\n",
    "for step in range(max_seq_len+1):\n",
    "    logits = nlx_gpt(image=img, input_ids=input_ids, segment_ids=segment_ids)\n",
    "    \n",
    "    logits = logits / temperature if temperature > 0 else 1.0\n",
    "    filtered_logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
    "    probs = F.softmax(filtered_logits, dim=-1)\n",
    "    prev = torch.multinomial(probs, dim=-1) if do_sample else torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\n",
    "    \n",
    "    # for explanation\n",
    "    one_hot = F.one_hot(prev, num_classes=logits.shape[-1]).type(torch.float32)\n",
    "    one_hot = one_hot.requires_grad_(True).cuda()\n",
    "    one_hot = torch.sum(one_hot.cuda()*logits, dim=-1)\n",
    "    \n",
    "    nlx_gpt.zero_grad()\n",
    "    \n",
    "    # image relevance\n",
    "    R = torch.eye(num_tokens, num_tokens, dtype=image_attn_blocks[0].attn_probs.dtype).to(device)\n",
    "    R = R.unsqueeze(0).expand(batch_size, num_tokens, num_tokens)\n",
    "    for i, blk in enumerate(image_attn_blocks):\n",
    "        if i < start_layer:\n",
    "            continue\n",
    "        grad = torch.autograd.grad(one_hot, [blk.attn_probs], retain_graph=True)[0].detach()\n",
    "        cam = blk.attn_probs.detach()\n",
    "        cam = cam.reshape(-1, cam.shape[-1], cam.shape[-1])\n",
    "        grad = grad.reshape(-1, grad.shape[-1], grad.shape[-1])\n",
    "        cam = grad * cam\n",
    "        cam = cam.reshape(batch_size, -1, cam.shape[-1], cam.shape[-1])\n",
    "        cam = cam.clamp(min=0).mean(dim=1)\n",
    "        R = R + torch.bmm(cam, R)\n",
    "    image_relevance = R[:,1,1:]\n",
    "    img_relevance_maps.append(image_relevance)\n",
    "    \n",
    "    if prev.item() in special_tokens_ids:\n",
    "        break\n",
    "    \n",
    "    # take care of when to start the <explanation> token\n",
    "    if not always_exp:\n",
    "        if prev.item()!=because_token_id:\n",
    "            new_segment = special_tokens_ids[-2] # answer segment\n",
    "        else:\n",
    "            new_segment = special_tokens_ids[-1] # explanation segment\n",
    "            always_exp = True\n",
    "    else:\n",
    "        new_segment = special_tokens_ids[-1] # explanation segment\n",
    "    \n",
    "    new_segment = torch.LongTensor([new_segment]).to(device)\n",
    "    current_output.append(prev)\n",
    "    current_logits.append(logits.unsqueeze(1))\n",
    "    input_ids = torch.cat((input_ids, prev), dim=1)\n",
    "    segment_ids = torch.cat((segment_ids, new_segment.unsqueeze(0).expand(segment_ids.shape[0],-1)), dim=1)\n",
    "\n",
    "current_output = torch.cat(current_output, dim=1)\n",
    "current_logits = torch.cat(current_logits, dim=1)\n",
    "current_output = current_output.detach().cpu().numpy()\n",
    "current_output = tokenizer.decode(current_output[0], clean_up_tokenization_space=True)\n",
    "print(current_output)\n",
    "pil_image = Image.open(img_path)\n",
    "show_image_relevance(img_relevance_maps[0], img, pil_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R = R / torch.max(R)\n",
    "R = R.squeeze(0)\n",
    "plt.imshow(R.detach().cpu().numpy())\n",
    "plt.colorbar()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
